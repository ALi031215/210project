---
title: "Predicting NBA Rookie Success from College Performance"
author: "Muhammad Shah, Alex Li"
date: "2024-04-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(ggplot2)
```

## Introduction

Predictive modeling of NBA rookie success based on college performance statistics provides crucial insights for sports analytics. This analysis utilizes historical data to establish relationships between collegiate achievements and professional performance in the NBA, thereby supporting decision-making in sports management.

## Research Question

How can NBA rookie success, measured by points per game, be predicted from their college basketball statistics? This study employs statistical modeling techniques to forecast NBA outcomes from detailed collegiate performance metrics.

## Data Description

This study employs two primary datasets:

1. **College Basketball Dataset**: Consists of detailed player statistics from NCAA college basketball spanning 2009 to 2021. This dataset includes metrics such as player efficiency ratings and scoring averages. 

[Source: https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data]
2. **NBA Player Statistics**: Contains performance data for NBA rookies' first season from 2009 to 2021, focusing on points per game and efficiency metrics. 

[Source: https://www.kaggle.com/datasets/raunakpandey030/nba-player-stats]

### Key Variables:
- `College_points`: Average points scored per game in the player's final college season.
- `eFG%`: Effective field goal percentage in college.
- `TS%`: True shooting percentage in college.
- `NBA_points`: Average points per game during the player's rookie NBA season.

### Data Cleaning and Merging Process
The data was meticulously cleaned to ensure consistency and relevance to the study's goals. This involved:
- Renaming columns for clarity.
- Filtering records to retain only the last year of college and the first year in the NBA to focus on the transition period.
- Removing duplicate entries and normalizing data formats for accurate merging.


```{r load-data, echo=TRUE, message=FALSE, warning=FALSE}
# Load libs
library(dplyr)
library(readr)
library(stringr)

# Reading and processing the college data
college <- read_csv("CollegeBasketballPlayers2009-2021.csv") %>%
  mutate(year = as.character(year)) %>%
  rename(College_points = pts) %>%
  arrange(desc(year)) %>%
  distinct(player_name, .keep_all = TRUE) %>%
  select(player_name, eFG, conf, TS_per, College_points, year)

# Reading and processing the NBA data
nba <- read_csv("NBA_Player_Stats.csv") %>%
  mutate(Year = as.character(Year),
         Year = str_split(Year, '-', simplify = TRUE)[, 1]) %>%
  arrange(Year) %>%
  rename(NBA_points = PTS,
         player_name = Player) %>%
  distinct(player_name, .keep_all = TRUE) %>%
  select(player_name, NBA_points)

# merging
merged_points <- inner_join(college, nba, by = "player_name")
```

###Exploratory Data Analysis

```{r}
library(ggplot2)

# Histogram of NBA Points
ggplot(merged_points, aes(x = NBA_points)) +
  geom_histogram(bins = 30, fill = "steelblue") +
  labs(title = "Distribution of NBA Rookie Points", x = "NBA Points per Game", y = "Frequency")

# Scatter plot of College Points vs. NBA Points
ggplot(merged_points, aes(x = College_points, y = NBA_points)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Relationship between College Points and NBA Rookie Points", x = "College Points per Game", y = "NBA Points per Game")
```
The exploratory data analysis (EDA) in this study focuses on understanding the distribution of the key response variable, NBA points per game, as well as the relationship between college performance metrics and NBA success. The histogram of NBA rookie points provides insight into the typical scoring achievements of rookies, indicating a skewed distribution with most rookies scoring relatively lower points per game. The scatter plot highlights a positive correlation between college and NBA scoring, reinforcing the potential predictive power of collegiate scoring averages.

Additionally, the boxplot of NBA points by college conference reveals variation in rookie success based on the conference from which players graduated, suggesting that the competitive level in different conferences may affect player readiness for the NBA. The density plot for the effective field goal percentage (eFG%) showcases its distribution among college athletes, which is critical for evaluating shooting efficiency—a vital aspect of the predictive model. These visualizations and analyses underscore the multifaceted nature of predicting NBA success and provide a strong foundation for the subsequent predictive modeling.

## Methodology

### Model Selection Process

The primary objective of this study is to predict NBA rookie success based on their college performance statistics. Given the nature of our response variable (NBA points per game, which is continuous), regression models were considered most appropriate for this analysis.

#### Initial Model Choices
We initially considered multiple regression models to handle the continuous nature of our target variable:
- **Linear Regression**: Chosen for its simplicity and interpretability.
- **Multilayer Perceptron (MLP) Regressor**: Selected for its ability to capture non-linear relationships that might exist between the predictors and the target.

### Predictor Variables
The predictor variables initially included were:
- `eFG%` (Effective Field Goal Percentage)
- `TS%` (True Shooting Percentage)
- `College_points` (Average points per game during the player's final college season)

These variables were chosen due to their direct relationship with scoring performance, which is our primary measure of NBA success.

### Model Enhancement with One-Hot Encoding
To improve the model’s performance, we introduced a categorical variable:
- `conf` (Conference): This variable represents the college conference of each player, which could have an influence on the player's transition to the NBA due to varying levels of competition across conferences.

One-hot encoding was applied to this categorical variable to convert it into a numerical format that could be used in our regression models. This allowed us to examine the impact of different conferences on NBA success.
```{r}
# One-hot encoding for the 'conf' variable
train_data <- model.matrix(~ conf - 1, data = train_data) %>% as.data.frame() %>%
  bind_cols(train_data[, c("eFG", "TS_per", "College_points", "NBA_points")])

test_data <- model.matrix(~ conf - 1, data = test_data) %>% as.data.frame() %>%
  bind_cols(test_data[, c("eFG", "TS_per", "College_points", "NBA_points")])

```

### Data Splitting
The data was split into training and test sets with a 70-30 ratio, using a random seed for reproducibility (`random_state=999`). This split was chosen to provide a substantial training set for model development while leaving a sizable portion of data for testing model validity.
```{r}
set.seed(999)
training_indices <- createDataPartition(merged_points$NBA_points, p = 0.7, list = FALSE)
train_data <- merged_points[training_indices, ]
test_data <- merged_points[-training_indices, ]
```


### Model Fitting and Diagnostics
Each model was trained on the training data set:
- **Linear Regression and MLP Regressor**: Both models were fitted using their respective methods from the `scikit-learn` library.
```{r}
linear_model <- lm(NBA_points ~ eFG + TS_per + College_points, data = train_data)
summary(linear_model)

# Load the required library
library(nnet)

# Fit an MLP model using the nnet function for regression, with reduced verbosity
mlp_model <- nnet(NBA_points ~ eFG + TS_per + College_points, 
                  data = train_data, 
                  size = 5,        # Number of units in the hidden layer
                  decay = 0.1,     # Weight decay for regularization
                  maxit = 200,     # Maximum number of iterations
                  linout = TRUE,   # Linear output activation function
                  trace = FALSE)   # Set trace to FALSE to reduce output verbosity

summary(mlp_model)
```


#### Model Evaluation Metrics
To evaluate model performance, we calculated:
- Mean Squared Error (MSE)
- R-squared (R²)

These metrics provide insights into the accuracy and explanatory power of the models, respectively.
```{r}
# Linear model evaluation
predictions_lm <- predict(linear_model, newdata = test_data)
mse_lm <- mse(test_y, predictions_lm)
r2_lm <- R2(predictions_lm, test_y)

# MLP model evaluation
predictions_mlp <- predict(mlp_model, newdata = test_data, type = "raw")
mse_mlp <- mse(test_y, predictions_mlp)
r2_mlp <- R2(predictions_mlp, test_y)

model_performance <- data.frame(
  Model = c("Linear Regression", "MLP Regressor"),
  MSE = c(mse_lm, mse_mlp),
  R2 = c(r2_lm, r2_mlp)
)

# Print the performance table
print(model_performance)

```

```{r}
library(ggplot2)

# Assuming 'predictions_lm' and 'predictions_mlp' are already created from previous steps

# Creating a data frame for plotting
actual_vs_predicted_lm <- data.frame(
  Actual = test_data$NBA_points,
  Predicted = predictions_lm,
  Model = 'Linear Regression'
)

actual_vs_predicted_mlp <- data.frame(
  Actual = test_data$NBA_points,
  Predicted = predictions_mlp,
  Model = 'MLP Regressor'
)

# Combining both data frames
all_predictions <- rbind(actual_vs_predicted_lm, actual_vs_predicted_mlp)

# Plotting Actual vs Predicted Values for Both Models
ggplot(all_predictions, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point(alpha = 0.5) +  # Using semi-transparent points
  geom_smooth(method = "lm", se = FALSE, color = "black") +  # Adding a regression line without confidence interval
  facet_wrap(~ Model) +  # Separate plots for each model
  labs(title = "Actual vs Predicted NBA Points",
       x = "Actual NBA Points per Game",
       y = "Predicted NBA Points per Game") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "green"))  # Custom colors for each model

```


### Model Diagnostics and Validation
After fitting the models, we conducted diagnostics to check for:
- **Homoscedasticity**: Ensuring that the residuals have constant variance across predictions.
- **Normality of Residuals**: Checked using Q-Q plots and Shapiro-Wilk tests.

Violations of model assumptions were addressed by considering transformation of predictor variables and adding interaction terms where necessary.
```{r}
# Residual plot for Linear Model
plot(residuals(linear_model), type = 'p', main = "Residual Plot for Linear Model")

# Checking normality of residuals
shapiro.test(residuals(linear_model))

```

### Final Model Selection
The final model selection was based on a combination of:
- Performance metrics (lower MSE and higher R²)
- Diagnostic results
- Simplicity and interpretability for stakeholders

```{r}
# Load necessary libraries
library(caret)
library(nnet)  # For MLP
library(dplyr)
library(ModelMetrics)  # For mse and r2_score

# Assume merged_points is already in your environment
set.seed(999)
training_indices <- createDataPartition(merged_points$NBA_points, p = 0.7, list = FALSE)
train_data <- merged_points[training_indices, ]
test_data <- merged_points[-training_indices, ]

# Prepare data for modeling
train_x <- train_data[, c("eFG", "TS_per", "College_points")]
train_y <- train_data$NBA_points
test_x <- test_data[, c("eFG", "TS_per", "College_points")]
test_y <- test_data$NBA_points

```

```{r}
linear_model <- lm(NBA_points ~ eFG + TS_per + College_points, data = train_data)
summary(linear_model)

# Predictions and model evaluation
predictions_lm <- predict(linear_model, newdata = test_data)
mse_lm <- mse(test_y, predictions_lm)
r2_lm <- R2(predictions_lm, test_y)
print(paste("Linear Model Test MSE:", mse_lm))
print(paste("Linear Model Test R2:", r2_lm))

```





```{r}
# Residual plot for Linear Model
plot(residuals(linear_model), type = 'p', main = "Residual Plot for Linear Model")

# Checking normality of residuals
shapiro.test(residuals(linear_model))
```

